# 索引构建与优化

## 1. 向量嵌入（Embedding）

### 1.1. 定义

* 定义：将复杂、高维的数据对象（如文本、图像、音视频等）转换为易处理、低维、稠密的连续数值向量的技术。输出将会是有固定长度的一维数组，长度通常在几百到几千之间。

* 向量空间的语义表示
  Embedding 产生的向量不是随机数值的堆砌，而是对数据**语义**的数学编码。
  * 核心原则：**相近的语义**在向量空间中**相近**。
  * 关键度量：衡量向量间的**距离**或**相似度**
    * **余弦相似度**（cosine similarity）【最常用】：计算两个向量夹角的余弦值。值域为[-1,1]，值越接近1，表示两个向量越相似。
    * **点积（Dot Product)**: 计算两个向量内积的大小。值越大，表示两个向量越相似。向量归一化后，点积等价于余弦相似度。
    * **欧氏距离**（Euclidean distance）：计算两个向量在空间中的直线距离。距离越小，语义越相似。

### 1.2 Embedding 在 RAG 中的作用

* 语义检索的基础
  RAG 检索的核心通常是基于Embedding的**语义检索**，通用流程为：
  1. 离线索引构建：切分知识库文档，通过Embedding模型将文本块转化为向量，并构建索引存入专门的向量数据库中。
  2. 在线查询检索：用户输入问题，通过Embedding模型将问题转化为向量。
  3. 相似度计算：在向量数据库中，计算问题向量与索引中向量的相似度，并排序。
  4. 召回上下文：选取相似度最高的Top-K个文档快，作为补充的上下文信息，与用户问题一同送到大语言模型（LLM）生成最终答案。
* 决定检索质量的关键
  Embedding 的质量直接决定了RAG检索召回内容的准确性与相关性。优秀的模型，即使问题表述不一致，亦能精准捕捉问题的语义，从而召回最相关的答案。

### 1.3 Embedding 技术发展

#### 1.3.1 静态词嵌入：上下文无关的表示

为词汇表中的每个单词生成一个固定的、与上下文无关的向量。比如“苹果”在“吃苹果”和“苹果手机”中的向量是相同的。

* 代表模型：Word2Vec (2013), GloVe (2014)
   * **Word2Vec**：将词映射为固定长度的向量，通过训练词向量，捕捉词与词之间的语义关系。Word2Vec有两种训练架构：Skip-gram 和 CBOW。
     
   * **GloVe**：全局向量（Global Vectors for Word Representation），通过全局统计词与词之间的共现矩阵信息，训练词向量。

* 局限性： 无法捕捉语言的深层语境信息，是动态模型要解决的核心问题。

#### 1.3.2 动态上下文相关的词嵌入模型（Contextual Word Embedding）

2017年，Transformer 架构的诞生带来了自注意力机制（Self-Attention），它允许模型在生成一个词的向量时，动态地考虑句子中所有其他词的影响。基于此，2018年 BERT 模型利用 Transformer 的编码器，通过掩码语言模型（MLM）等自监督任务进行预训练，生成了深度上下文相关的嵌入。同一个词在不同语境中会生成不同的向量，这有效解决了静态嵌入的一词多义难题。


动态上下文嵌入模型核心发展脉络如下图所示：
![动态上下文核心发展脉络](../../data/C3/imgs/emdebdding_timeline.png)

## 2.  多模态嵌入

## 3. 向量数据库

## 4. Milvus实践

## 5. 索引优化