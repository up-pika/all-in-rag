# 索引构建与优化

## 1. 向量嵌入（Embedding）

### 1.1. 定义

* 定义：将复杂、高维的数据对象（如文本、图像、音视频等）转换为易处理、低维、稠密的连续数值向量的技术。输出将会是有固定长度的一维数组，长度通常在几百到几千之间。

* 向量空间的语义表示
  Embedding 产生的向量不是随机数值的堆砌，而是对数据**语义**的数学编码。
  * 核心原则：**相近的语义**在向量空间中**相近**。
  * 关键度量：衡量向量间的**距离**或**相似度**
    * **余弦相似度**（cosine similarity）【最常用】：计算两个向量夹角的余弦值。值域为[-1,1]，值越接近1，表示两个向量越相似。
    * **点积（Dot Product)**: 计算两个向量内积的大小。值越大，表示两个向量越相似。向量归一化后，点积等价于余弦相似度。
    * **欧氏距离**（Euclidean distance）：计算两个向量在空间中的直线距离。距离越小，语义越相似。

### 1.2 Embedding 在 RAG 中的作用

* 语义检索的基础
  RAG 检索的核心通常是基于Embedding的**语义检索**，通用流程为：
  1. 离线索引构建：切分知识库文档，通过Embedding模型将文本块转化为向量，并构建索引存入专门的向量数据库中。
  2. 在线查询检索：用户输入问题，通过Embedding模型将问题转化为向量。
  3. 相似度计算：在向量数据库中，计算问题向量与索引中向量的相似度，并排序。
  4. 召回上下文：选取相似度最高的Top-K个文档快，作为补充的上下文信息，与用户问题一同送到大语言模型（LLM）生成最终答案。
* 决定检索质量的关键
  Embedding 的质量直接决定了RAG检索召回内容的准确性与相关性。优秀的模型，即使问题表述不一致，亦能精准捕捉问题的语义，从而召回最相关的答案。

### 1.3 Embedding 技术发展

#### 1.3.1 静态词嵌入：上下文无关的表示

为词汇表中的每个单词生成一个固定的、与上下文无关的向量。比如“苹果”在“吃苹果”和“苹果手机”中的向量是相同的。

* 代表模型：Word2Vec (2013), GloVe (2014)
   * **Word2Vec**：将词映射为固定长度的向量，通过训练词向量，捕捉词与词之间的语义关系。Word2Vec有两种训练架构：Skip-gram 和 CBOW。
     
   * **GloVe**：全局向量（Global Vectors for Word Representation），通过全局统计词与词之间的共现矩阵信息，训练词向量。

* 局限性： 无法捕捉语言的深层语境信息，是动态模型要解决的核心问题。

#### 1.3.2 动态上下文相关的词嵌入模型（Contextual Word Embedding）
为每个词生成多个向量，这些向量根据上下文的不同而变化。例如：

* **BERT**：双向编码器表示（Bidirectional Encoder Representations from Transformers），通过预训练和微调，生成上下文相关的词向量。

* **GPT**：生成预训练变换器（Generative Pre-trained Transformer），通过预训练和微调，生成上下文相关的词向量。
动态上下文嵌入模型核心发展脉络如下图所示：
![动态上下文核心发展脉络](../data/C3/动态上下文核心发展脉络.png)

1. 多模态嵌入模型（Multimodal Embedding）
将多种模态（如文本、图像、音频等）的数据融合在一起，生成统一的向量表示。例如：

## 2.  多模态嵌入

## 3. 向量数据库

## 4. Milvus实践

## 5. 索引优化