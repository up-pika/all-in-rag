# 索引构建与优化

## 1. 向量嵌入（Embedding）

### 1.1. 定义

* 定义：将复杂、高维的数据对象（如文本、图像、音视频等）转换为易处理、低维、稠密的连续数值向量的技术。输出将会是有固定长度的一维数组，长度通常在几百到几千之间。

* 向量空间的语义表示
  Embedding 产生的向量不是随机数值的堆砌，而是对数据**语义**的数学编码。
  * 核心原则：**相近的语义**在向量空间中**相近**。
  * 关键度量：衡量向量间的**距离**或**相似度**
    * **余弦相似度**（cosine similarity）【最常用】：计算两个向量夹角的余弦值。值域为[-1,1]，值越接近1，表示两个向量越相似。
    * **点积（Dot Product)**: 计算两个向量内积的大小。值越大，表示两个向量越相似。向量归一化后，点积等价于余弦相似度。
    * **欧氏距离**（Euclidean distance）：计算两个向量在空间中的直线距离。距离越小，语义越相似。

### 1.2 Embedding 在 RAG 中的作用

1. 语义检索的基础
  RAG 检索的核心通常是基于Embedding的**语义检索**，通用流程为：
   1. **离线索引构建**：切分知识库文档，通过Embedding模型将文本块转化为向量，并构建索引存入专门的向量数据库中。
   2. **在线查询检索**：用户输入问题，通过Embedding模型将问题转化为向量。
   3. **相似度计算**：在向量数据库中，计算问题向量与索引中向量的相似度，并排序。
   4. **召回上下文**：选取相似度最高的Top-K个文档快，作为补充的上下文信息，与用户问题一同送到大语言模型（LLM）生成最终答案。
2. 决定检索质量的关键
  Embedding 的质量直接决定了RAG检索召回内容的准确性与相关性。优秀的模型，即使问题表述不一致，亦能精准捕捉问题的语义，从而召回最相关的答案。

### 1.3 Embedding 技术发展

#### 1.3.1 静态词嵌入：上下文无关的表示

为词汇表中的每个单词生成一个固定的、与上下文无关的向量。比如“苹果”在“吃苹果”和“苹果手机”中的向量是相同的。

* 代表模型：Word2Vec (2013), GloVe (2014)
   * **Word2Vec**：将词映射为固定长度的向量，通过训练词向量，捕捉词与词之间的语义关系。Word2Vec有两种训练架构：Skip-gram 和 CBOW。
     
   * **GloVe**：全局向量（Global Vectors for Word Representation），通过全局统计词与词之间的共现矩阵信息，训练词向量。

* 局限性： 无法捕捉语言的深层语境信息，是动态模型要解决的核心问题。

#### 1.3.2 动态上下文相关的词嵌入模型（Contextual Word Embedding）

2017年，Transformer 架构的诞生带来了自注意力机制（Self-Attention），它允许模型在生成一个词的向量时，动态地考虑句子中所有其他词的影响。基于此，2018年 BERT 模型利用 Transformer 的编码器，通过掩码语言模型（MLM）等自监督任务进行预训练，生成了深度上下文相关的嵌入。同一个词在不同语境中会生成不同的向量，这有效解决了静态嵌入的一词多义难题。

动态上下文嵌入模型核心发展脉络如下图所示：
![动态上下文核心发展脉络](../../data/C3/imgs/emdebdding_timeline.png)

### 1.4 嵌入模型训练原理

当代主流嵌入模型的核心通常是 Transformer 的编码器（Encoder）部分，BERT 就是其中的典型代表。它通过堆叠多个 Transformer Encoder 层来构建一个深度的双向表示学习网络。BERT 的成功很大程度上归功于 自监督学习 策略，它允许模型从海量的、无标注的文本数据中学习知识。

#### 主要训练任务：
##### 任务一：掩码语言模型 (Masked Language Model, MLM)

- **过程**：
    1.  随机地将输入句子中 15% 的词元（Token）替换为一个特殊的 `[MASK]` 标记。
    2.  让模型去预测这些被遮盖住的原始词元是什么。
- **目标**：通过这个任务，模型被迫学习每个词元与其上下文之间的关系，从而掌握深层次的语境语义。

##### 任务二：下一句预测 (Next Sentence Prediction, NSP)

- **过程**：
    1.  构造训练样本，每个样本包含两个句子 A 和 B。
    2.  其中 50% 的样本，B 是 A 的真实下一句（IsNext）；另外 50% 的样本，B 是从语料库中随机抽取的句子（NotNext）。
    3.  让模型判断 B 是否是 A 的下一句。
- **目标**：这个任务让模型学习句子与句子之间的逻辑关系、连贯性和主题相关性。
- **重要说明**：后续的研究（如 RoBERTa）发现[^1]，NSP 任务可能过于简单，甚至会损害模型性能。因此，许多现代的预训练模型（如 RoBERTa、SBERT）已经放弃了 NSP 任务。

### 1.5 效果增强策略

虽然 MLM 和 NSP 赋予了模型强大的基础语义理解能力，但为了在检索任务中表现更佳，现代嵌入模型通常会引入更具针对性的训练策略。

- **度量学习 (Metric Learning)** ：
    - **思想**：直接以“相似度”作为优化目标。
    - **方法**：收集大量相关的文本对（例如，（问题，答案）、（新闻标题，正文））。训练的目标是优化向量空间中的**相对距离**：让“正例对”的向量表示在空间中被“拉近”，而“负例对”的向量表示被“推远”。关键在于优化排序关系，而非追求绝对的相似度值（如 1 或 0），因为过度追求极端值可能导致模型过拟合。

- **对比学习 (Contrastive Learning)** ：
    - **思想**：在向量空间中，将相似的样本“拉近”，将不相似的样本“推远”。
    - **方法**：构建一个三元组（Anchor, Positive, Negative）。其中，Anchor 和 Positive 是相关的（例如，同一个问题的两种不同问法），Anchor 和 Negative 是不相关的。训练的目标是让 `distance(Anchor, Positive)` 尽可能小，同时让 `distance(Anchor, Negative)` 尽可能大。

### 1.6 嵌入模型选型指南
1. 从HuggingFace的[MTEB (Massive Text Embedding Benchmark) ](https://huggingface.co/spaces/mteb/leaderboard) 排行榜中选取备选模型，快速筛选掉大量不合适的模型。
2. 根据任务需求和硬件条件，选择合适的模型。例如，对于需要快速部署的应用，可以选择较小的模型；对于需要高精度的应用，可以选择较大的模型。
  

### Demo代码理解



## 参考文献

[^1]: [*RoBERTa: A Modified BERT Model for NLP*](https://www.comet.com/site/blog/roberta-a-modified-bert-model-for-nlp/)
