# 多模态嵌入


## Demo代码理解

```python
import torch
from visual_bge.visual_bge.modeling import Visualized_BGE

model = Visualized_BGE(model_name_bge="BAAI/bge-base-en-v1.5",
                      model_weight="../../models/bge/Visualized_base_en_v1.5.pth")
model.eval()

with torch.no_grad():
    text_emb = model.encode(text="blue whale")
    img_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png")
    multi_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png", text="blue whale")
    img_emb_2 = model.encode(image="../../data/C3/imgs/datawhale02.png")
    multi_emb_2 = model.encode(image="../../data/C3/imgs/datawhale02.png", text="blue whale")

# 计算相似度
sim_1 = img_emb_1 @ img_emb_2.T
sim_2 = img_emb_1 @ multi_emb_1.T
sim_3 = text_emb @ multi_emb_1.T
sim_4 = multi_emb_1 @ multi_emb_2.T

print("=== 相似度计算结果 ===")
print(f"纯图像 vs 纯图像: {sim_1}")
print(f"图文结合1 vs 纯图像: {sim_2}")
print(f"图文结合1 vs 纯文本: {sim_3}")
print(f"图文结合1 vs 图文结合2: {sim_4}")

# 向量信息分析
print("\n=== 嵌入向量信息 ===")
print(f"多模态向量维度: {multi_emb_1.shape}")
print(f"图像向量维度: {img_emb_1.shape}")
print(f"多模态向量示例 (前10个元素): {multi_emb_1[0][:10]}")
print(f"图像向量示例 (前10个元素):   {img_emb_1[0][:10]}")

```
执行结果如下：
```Bash
tokenizer_config.json: 100%|████████████████████████████| 366/366 [00:00<00:00, 122kB/s] 
vocab.txt: 232kB [00:00, 719kB/s]
special_tokens_map.json: 100%|█████████████████████████████████| 125/125 [00:00<?, ?B/s] 
tokenizer.json: 711kB [00:00, 2.98MB/s]
=== 相似度计算结果 ===
纯图像 vs 纯图像: tensor([[0.8318]])
图文结合1 vs 纯图像: tensor([[0.8291]])
图文结合1 vs 纯文本: tensor([[0.7627]])
图文结合1 vs 图文结合2: tensor([[0.9058]])

=== 嵌入向量信息 ===
多模态向量维度: torch.Size([1, 768])
图像向量维度: torch.Size([1, 768])
多模态向量示例 (前10个元素): tensor([ 0.0360, -0.0032, -0.0377,  0.0240,  0.0140,  0.0340,  0.0148,  0.0292,
         0.0060, -0.0145])
图像向量示例 (前10个元素):   tensor([ 0.0407, -0.0606, -0.0037,  0.0073,  0.0305,  0.0318,  0.0132,  0.0442,
        -0.0380, -0.0270])
```
结果解析：
1. 代码使用`Visualized_BGE`同时对文本、图像以及图文对进行编码，并进行矩阵乘法操作。由于`Visualized_BGE`默认归一化向量，故矩阵乘法的结果等同于余弦相似度。值越接近1，表示语义越接近。
2. 纯图像 vs 纯图像 ≈ 0.83，说明两张 datawhale logo 图的视觉语义很接近。图文结合1 vs 纯图像 ≈ 0.83 与 纯图像 vs 纯图像接近，说明当把文字“blue whale”与第一张图拼起来编码时，模型仍然与另一张纯图保持高度一致。
3. 图文结合1 vs 纯文本 ≈ 0.76 略低一些，符合预期：纯文本只包含语义描述，没有视觉细节；但它仍与图文混合向量高度相关
4. 图文结合1 vs 图文结合2 ≈ 0.91 是最高的，因为两个图文向量都包含了相同的文本提示，并且图像也极其相似。
5. 嵌入向量维度都是 768，说明模型输出在同一个向量空间里，可直接用向量距离或检索库（FAISS、Milvus 等）做相似度搜索

### 实验-替换部分文本
1. 将datawhale开源组织的logo替换为blue whale后，输出结果与未修改前保持一致。
结果分析如下：
多模态嵌入中图像`token`数量远多于文字token，`encode`模型先拼接图像特征再拼短文本，最后再用`sentense_embedding`池化为单个768维向量；在这种架构下，图像模态占主导，单句英文提示只做轻微调制。所以只要图文仍描述“鲸”这一语义（与图片一致），最终向量主要由图像决定，相似度不会有太大变化。

2. 将datawhale开源组织的logo替换为与图片内容无关的文本，如“红房子里面住着可爱的猫”后，运行结果如下：
   ``` Bash
   
   ```