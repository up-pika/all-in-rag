# 多模态嵌入（Multimodel Embedding）
多模态嵌入能打破不同模态在向量空间中的“墙”，将不同类型的数据（如图像和文本）映射到**同一个共享的向量空间**。在这个统一的空间里，一段描述“一只奔跑的狗”的文字，其向量会非常接近一张真实小狗奔跑的图片向量。

实现这一目标的关键，在于解决 **跨模态对齐 (Cross-modal Alignment)** 的挑战。以对比学习、视觉 Transformer (ViT) 等技术为代表的突破，让模型能够学习到不同模态数据之间的语义关联，最终催生了像 CLIP 这样的模型。


## 1. 理解CLIP模型

CLIP 模型是一种由`OpenAI`在2021年发布的多模态预训练模型，全称为 Contrastive Language–Image Pre-Training（对比语言-图像预训练），它为多模态嵌入定义了一个有效的范式。

CLIP 模型使用一种对比学习策略，通过**最大化正确图文对的向量相似度，同时最小化错误配对的相似度**，来学习图像和文本之间的关联。通过这种“**拉近正例，推远负例**”的方式，模型从海量数据中学会了将语义相关的图像和文本在向量空间中拉近。通过在大规模图文对上进行对比学习，将图像和文本映射到同一向量空间，从而实现**跨模态对齐**与强大的**零样本（zero-shot）**能力，能直接进行图像分类和文本检索，无需额外的微调。将传统的分类任务转化为“图文检索”问题，如，判断一张图片是不是猫，只需计算图片向量与“a photo of cat”文本向量的相似度即可。

### 1.1 CLIP的工作原理

* **对比学习**： CLIP模型不直接学习硬标签（如“这张图片是猫”），而是学习使“这张图片的内容与‘一只猫的照片’这段文字最匹配”。
* **多模态对齐**： 它通过对比学习，将描述相似概念的图像和文本嵌入到同一个高维向量空间中，使它们在向量空间中靠得更近。
* **双编码器架构**： 模型包含一个图像编码器（如ViT或CNN）和一个文本编码器（Transformer），它们将输入数据转化为向量表示

## Demo代码理解

```python
import torch
from visual_bge.visual_bge.modeling import Visualized_BGE

model = Visualized_BGE(model_name_bge="BAAI/bge-base-en-v1.5",
                      model_weight="../../models/bge/Visualized_base_en_v1.5.pth")
model.eval()

with torch.no_grad():
    text_emb = model.encode(text="blue whale")
    img_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png")
    multi_emb_1 = model.encode(image="../../data/C3/imgs/datawhale01.png", text="blue whale")
    img_emb_2 = model.encode(image="../../data/C3/imgs/datawhale02.png")
    multi_emb_2 = model.encode(image="../../data/C3/imgs/datawhale02.png", text="blue whale")

# 计算相似度
sim_1 = img_emb_1 @ img_emb_2.T
sim_2 = img_emb_1 @ multi_emb_1.T
sim_3 = text_emb @ multi_emb_1.T
sim_4 = multi_emb_1 @ multi_emb_2.T

print("=== 相似度计算结果 ===")
print(f"纯图像 vs 纯图像: {sim_1}")
print(f"图文结合1 vs 纯图像: {sim_2}")
print(f"图文结合1 vs 纯文本: {sim_3}")
print(f"图文结合1 vs 图文结合2: {sim_4}")

# 向量信息分析
print("\n=== 嵌入向量信息 ===")
print(f"多模态向量维度: {multi_emb_1.shape}")
print(f"图像向量维度: {img_emb_1.shape}")
print(f"多模态向量示例 (前10个元素): {multi_emb_1[0][:10]}")
print(f"图像向量示例 (前10个元素):   {img_emb_1[0][:10]}")

```
执行结果如下：
```Bash
tokenizer_config.json: 100%|████████████████████████████| 366/366 [00:00<00:00, 122kB/s] 
vocab.txt: 232kB [00:00, 719kB/s]
special_tokens_map.json: 100%|█████████████████████████████████| 125/125 [00:00<?, ?B/s] 
tokenizer.json: 711kB [00:00, 2.98MB/s]
=== 相似度计算结果 ===
纯图像 vs 纯图像: tensor([[0.8318]])
图文结合1 vs 纯图像: tensor([[0.8291]])
图文结合1 vs 纯文本: tensor([[0.7627]])
图文结合1 vs 图文结合2: tensor([[0.9058]])

=== 嵌入向量信息 ===
多模态向量维度: torch.Size([1, 768])
图像向量维度: torch.Size([1, 768])
多模态向量示例 (前10个元素): tensor([ 0.0360, -0.0032, -0.0377,  0.0240,  0.0140,  0.0340,  0.0148,  0.0292,
         0.0060, -0.0145])
图像向量示例 (前10个元素):   tensor([ 0.0407, -0.0606, -0.0037,  0.0073,  0.0305,  0.0318,  0.0132,  0.0442,
        -0.0380, -0.0270])
```
结果解析：
1. 代码使用`Visualized_BGE`同时对文本、图像以及图文对进行编码，并进行矩阵乘法操作。由于`Visualized_BGE`默认归一化向量，故矩阵乘法的结果等同于余弦相似度。值越接近1，表示语义越接近。
2. 纯图像 vs 纯图像 ≈ 0.83，说明两张 datawhale logo 图的视觉语义很接近。图文结合1 vs 纯图像 ≈ 0.83 与 纯图像 vs 纯图像接近，说明当把文字“blue whale”与第一张图拼起来编码时，模型仍然与另一张纯图保持高度一致。
3. 图文结合1 vs 纯文本 ≈ 0.76 略低一些，符合预期：纯文本只包含语义描述，没有视觉细节；但它仍与图文混合向量高度相关
4. 图文结合1 vs 图文结合2 ≈ 0.91 是最高的，因为两个图文向量都包含了相同的文本提示，并且图像也极其相似。
5. 嵌入向量维度都是 768，说明模型输出在同一个向量空间里，可直接用向量距离或检索库（FAISS、Milvus 等）做相似度搜索

### 实验-替换部分文本
1. 将datawhale开源组织的logo替换为blue whale后，输出结果与未修改前保持一致。
结果分析如下：
多模态嵌入中图像`token`数量远多于文字token，`encode`模型先拼接图像特征再拼短文本，最后再用`sentense_embedding`池化为单个768维向量；在这种架构下，图像模态占主导，单句英文提示只做轻微调制。所以只要图文仍描述“鲸”这一语义（与图片一致），最终向量主要由图像决定，相似度不会有太大变化。

2. 将datawhale开源组织的logo替换为与图片内容无关的文本，如“红房子里面住着可爱的猫”后，运行结果如下：
   ``` Bash
   === 相似度计算结果 ===
   纯图像 vs 纯图像: tensor([[0.8318]])
   图文结合1 vs 纯图像: tensor([[0.9060]])
   图文结合1 vs 纯文本: tensor([[0.6123]])
   图文结合1 vs 图文结合2: tensor([[0.8665]])
   ```
结果分析：
1. 图文结合1 vs 纯图像=0.9060：文本虽然与图片内容无关，但文本对最终向量的扰动让其更加接近纯图像，由于所有向量最终都被 normalize，任意小的方向变化都可能让两个单位向量的点积上升或下降。
2. 图文结合1 vs 纯文本=0.6123：纯文本与文本+图像混合后的语义不再一致，相似度大幅下降。从图文结合1 vs 图文结合2=0.8665看，相较于0.9058只是略降，说明文本对最终向量的影响不太大，远小于图像的影响。