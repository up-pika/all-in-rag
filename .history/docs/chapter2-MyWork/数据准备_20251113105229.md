# 数据准备

## 1. 数据加载

使用Unstructured库加载解析多格式文档，如PDF、Word、Excel、HTML等。
示例代码中使用通用解析文档的函数-partition,并使用content_type字段区分并指定文档类型，若该参数数未设置，Unstructured 库会自动检测文件类型，并选择对应处理方案。
现使用专用函数 partition_pdf 对PDF文档进行解析，可发现针对示例网页PDF文档的解析结果与示例代码一致，代码如下：

```python
from collections import Counter

from unstructured.partition.pdf import partition_pdf

# PDF文件路径
pdf_path = "../../data/C2/pdf/rag.pdf"
# 输出文件路径
output_path = "../../data/C2/pdf/Unstructured_partition_pdf_parse.txt"

# 使用Unstructured加载并解析PDF文档
elements = partition_pdf(filename=pdf_path)

# 准备输出内容
output_lines = []
output_lines.append(
    f"解析完成: {len(elements)} 个元素, {sum(len(str(e)) for e in elements)} 字符\n"
)

# 统计元素类型
types = Counter(e.category for e in elements)
output_lines.append(f"元素类型: {dict(types)}\n")

# 显示所有元素
output_lines.append("\n所有元素:\n")
for i, element in enumerate(elements, 1):
    output_lines.append(f"Element {i} ({element.category}):\n")
    output_lines.append(str(element) + "\n")
    output_lines.append("=" * 60 + "\n")

# 将结果写入文件
with open(output_path, "w", encoding="utf-8") as f:
    f.writelines(output_lines)

# 同时打印到控制台
print("".join(output_lines))
print(f"\n结果已保存到: {output_path}")

```
解析结果见：[Unstructured_partition_pdf_parse.txt](../../data/C2/pdf/Unstructured_partition_pdf_parse.txt)

# 2. 文本分块

文本分块即将长篇文档切分成小块的，易于处理的单元，作为后续向量检索和模型处理的基本单位。

## 2.1 文本分块重要性
将长文本分解为适当大小的片段，需要考量：模型的**上下文限制**和检索生成的**性能需求**
主要考虑嵌入模型和LLM模型的上下文限制，过长的片段可能导致嵌入模型在将文本转成向量的过程中，出现文段阶段，从而导致信息缺失。另一方面，过长的片段也会导致检索生成的性能下降，因为需要处理更多的数据。

## 2.2 文本分块方法
以下几种方式均来自LangChain库。

### 2.2.1 固定大小分块

优点：实现简单、处理速度快且计算开销小。

缺点：可能会在语义边界处切断文本，影响内容的完整性和连贯性。实际的固定大小分块实现（如LangChain的 CharacterTextSplitter）通常会结合分隔符来减少这种问题，在段落边界处优先切分，只有在必要时才会强制按大小切断。因此，这种方法在日志分析、数据预处理等场景中仍有其应用价值。

### 2.2.2 递归字符分块

优点：有效平衡语义完整性与块大小控制。若切分后的块仍然过大，会继续对这个大块应用下一优先级分隔符（如句号），如此循环往复，直到块满足大小限制。这种分层处理的机制，能够在尽可能保持高级语义结构完整性的同时，有效控制块大小。

缺点：递归分块方法虽然能够较好地平衡语义完整性与块大小控制，但实现较为复杂，计算开销较大。此外，递归分块方法在处理长文本时，可能会产生大量的小块，这可能会对后续处理步骤（如向量检索、模型处理）造成额外的负担。

### 2.2.3 语义分块

核心为**在语义主题发生显著变化的地方进行切分**，使得每个分块有高度的内部语义一致性。

实现原理：
1. 句子分割：按标准规则切分文本为句子列表
2. 上下文感知嵌入：对每buffer_size个句子进行组合，对这个临时的，更长的组合文本进行嵌入，使得嵌入向量含有上下文语义
3. 计算语义距离：对每对相邻句子的嵌入向量之间计算余弦距离。距离越大，语义关联越弱，越有可能成为分块边界
4. 识别断点：SemanticChuncker 分析所有计算出的距离值，并选择一个统计方法(默认为 百分位法，可选标准差法、四分位距法、梯度法)来确定一个动态阈值。如百分位法，默认设定95,即距离值在95%以上的断点会被认为是分块边界
5. 合并成块：根据识别出的所有断点位置，切分原始句子序列，然后合并每个分块中的句子，形成最终的，语义连贯的文本块。

优点：
1. 语义连贯性：通过在语义主题发生显著变化的地方进行切分，能够确保每个分块内部具有高度的语义连贯性，从而提高模型处理和分析的准确性。
2. 自适应块大小：根据文本内容的语义变化，自适应地调整块大小，使得每个分块既不过大也不过小，从而在保证语义完整性的同时，也提高了处理效率。

缺点：
1. 实现复杂度较高：需要处理多个步骤，包括句子分割、上下文感知嵌入、语义距离计算、断点识别和块合并等，实现较为复杂。

### 2.2.4 基于文档结构的分块




